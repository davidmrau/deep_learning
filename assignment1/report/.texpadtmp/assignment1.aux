\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}MLP backprop and NumPy implementation}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Analytical derivation of gradients}{1}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}NumPy implementation}{2}{subsection.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Accuracy (in \%) and loss curves of the Numpy MLP for default values of parameters: batch size : 200, learning rate : 0.002 and dnn hidden units : 100. The top accuracy on the whole test set within the training was 47.24\%.\relax }}{2}{figure.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{numpy_mlp}{{1}{2}{Accuracy (in \%) and loss curves of the Numpy MLP for default values of parameters: batch size : 200, learning rate : 0.002 and dnn hidden units : 100. The top accuracy on the whole test set within the training was 47.24\%.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}PyTorch MLP}{3}{section.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Accuracy (in \%) and loss curves of the Pytorch MLP for parameters: batch size : 200, learning rate : 0.02, batch normalization, l1 regularization, dropout 0.2, with two hidden layers of size 200. The top accuracy on the whole test set within the training is 56.06\%.\relax }}{3}{figure.caption.8}}
\newlabel{tuned_pytorch}{{2}{3}{Accuracy (in \%) and loss curves of the Pytorch MLP for parameters: batch size : 200, learning rate : 0.02, batch normalization, l1 regularization, dropout 0.2, with two hidden layers of size 200. The top accuracy on the whole test set within the training is 56.06\%.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Custom Module: Batch Normalization}{3}{section.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}PyTorch CNN}{3}{section.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Accuracy (in \%) and loss curves of the Pytorch ConvNet for default values of parameters: batch size : 32, learning rate : 0.0001. The top accuracy on the whole test set within the training is 78\%.\relax }}{4}{figure.caption.11}}
\newlabel{convnet}{{3}{4}{Accuracy (in \%) and loss curves of the Pytorch ConvNet for default values of parameters: batch size : 32, learning rate : 0.0001. The top accuracy on the whole test set within the training is 78\%.\relax }{figure.caption.11}{}}
